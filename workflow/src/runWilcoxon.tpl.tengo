// run Wilcoxon test
self := import("@platforma-sdk/workflow-tengo:tpl")
exec := import("@platforma-sdk/workflow-tengo:exec")
assets:= import("@platforma-sdk/workflow-tengo:assets")
pt := import("@platforma-sdk/workflow-tengo:pt")

self.validateInputs({
	"__options__,closed": "",
	csvCounts: "any",
	csvCovariates: "any",
	numerators: "any",
	contrastFactorName: "any",
	params: {
		"__options__,closed": "",
		"denominator,omitempty": "string",
		"pAdjThreshold,omitempty": "number",
		"log2FCThreshold,omitempty": "number"
	}
	
})

self.defineOutputs("topTableCsv", "topTableFilteredCsv", "deErrorsCsv")

self.body(func(args) {
	csvCounts := args.csvCounts
	csvCovariates := args.csvCovariates
	numerators := args.numerators
	contrastFactorName := args.contrastFactorName
	
	denominator := args.params.denominator
	pAdjThreshold := args.params.pAdjThreshold
	log2FCThreshold := args.params.log2FCThreshold

	wf := pt.workflow().cpu(1).mem("2GiB")
	dfsTopTable := []
	dfsTopTableFiltered := []
	dfsDeErrors := []

	// We need a column type schema for cases where empty filtered tables are 
	// concatenated with non-empty ones
	// In these cases the column types of empty columns are String and fail to 
	//merge with same columns that have different type
	schema := [
		{column: "Contrast", type: "String"},
		{column: "gene", type: "String"},
		{column: "logfc", type: "Float"},
		{column: "pval", type: "Float"},
		{column: "pval_adj", type: "Float"},
		{column: "score", type: "Float"},
		{column: "minlog10padj", type: "Float"},
		{column: "regulationDirection", type: "String"}
	]
	schemaDeErrors := [
		{column: "Error", type: "String"},
		{column: "value", type: "String"}
	]
	for numerator in numerators {
		testWilcoxon := exec.builder().
			software(assets.importSoftware("@platforma-open/milaboratories.sc-differential-expression.software:run-deWilcoxon")).
			mem("16GiB").
			cpu(1).
			addFile("metadata.csv", csvCovariates).
			addFile("counts.csv", csvCounts).
			arg("--expr").arg("counts.csv").
			arg("--meta").arg("metadata.csv").
			arg("--group1").arg(string(numerator)).
			arg("--group2").arg(string(denominator)).
			arg("--condition_col").arg(string(contrastFactorName)).
			arg("--output").arg("topTable.csv").
			arg("--padj_cutoff").arg(string(pAdjThreshold)).
			arg("--logfc_cutoff").arg(string(log2FCThreshold)).
			arg("--error_output").arg("de_errors.csv").
			saveFile("topTable.csv").
			saveFile("topTable_filtered.csv").
			saveFile("de_errors.csv").
			printErrStreamToStdout().
			saveStdoutContent().
			cache(24 * 60 * 60 * 1000).
			run()
		// These tables might be empty, so we make sure to include an schema of expected column formats
		dfsTopTable += [wf.frame(testWilcoxon.getFile("topTable.csv"), {xsvType: "csv", schema: schema})]
		dfsTopTableFiltered += [wf.frame(testWilcoxon.getFile("topTable_filtered.csv"), {xsvType: "csv", schema: schema})]
		dfsDeErrors += [wf.frame(testWilcoxon.getFile("de_errors.csv"), {xsvType: "csv", schema: schemaDeErrors})]
	}

	///////// ptabler steps //////////
	concatenatedTopTable := pt.concat(dfsTopTable)
	concatenatedTopTable.save("concatenated_output_topTable.csv")
	concatenatedTopTableFiltered := pt.concat(dfsTopTableFiltered)
	concatenatedTopTableFiltered.save("concatenated_output_topTable_filtered.csv")

	// Concatenate and deduplicate error files
	concatenatedDeErrors := pt.concat(dfsDeErrors)
	// @TODO: find a better way to remove duplicated lines
	uniqueDeErrors := concatenatedDeErrors.groupBy("Error").agg(
		pt.col("value").first().alias("value")
	)
	uniqueDeErrors.save("concatenated_output_de_errors.csv")

	ptablerResult := wf.run()
	//////////////////

	// Return CSV file paths
	return {
		topTableCsv: ptablerResult.getFile("concatenated_output_topTable.csv"),
		topTableFilteredCsv: ptablerResult.getFile("concatenated_output_topTable_filtered.csv"),
		deErrorsCsv: ptablerResult.getFile("concatenated_output_de_errors.csv")
	}
})
